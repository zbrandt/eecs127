\section{Optimality Conditions}

\subsection{Gradient \& Hessian}
Consider a function $f(x): \R^n \to \R$ and assume $f(x)$ is
twice continuously differentiable. Let $x_i$ denote the
$i$-th entry of $x$ for $i = 1,\ldots,n$.

The \textbf{gradient} is an $n$-dimensional vector
$\nabla f(x) = \left(\frac{\partial f}{\partial x_1},
\frac{\partial f}{\partial x_2}, \ldots,
\frac{\partial f}{\partial x_n}\right)$.

The \textbf{Hessian} is an $n \times n$ symmetric matrix
$\nabla^2 f(x) = \begin{bmatrix} \frac{\partial^2 f}
{\partial x_1 \partial x_1} & \frac{\partial^2 f}
{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}
{\partial x_1 \partial x_n} \\ \frac{\partial^2 f}
{\partial x_2 \partial x_1} & \frac{\partial^2 f}
{\partial x_2 \partial x_2} & \cdots & \frac{\partial^2 f}
{\partial x_2 \partial x_n} \\ \vdots & \vdots & \ddots &
\vdots \\ \frac{\partial^2 f}{\partial x_n \partial x_1} &
\frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots &
\frac{\partial^2 f}{\partial x_n \partial x_n} \end{bmatrix}$.

If $n=1$, then the gradient is the first-order derivative
and the Hessian is the second-order derivative.

Suppose that $f(x)$ is quadratic, i.e.,
$f(x) = x^\top P x + q^\top x + r$ for some
$P \in \mathbb{S}^n$, $q \in \R^n$, and $r \in \R$. Then, it
holds that $\nabla f(x) = 2Px + q$ and
$\nabla^2 f(x) = 2P$.

\textbf{Gradient chain rule}: Consider functions
$f: \R^m \to \R$ and $g: \R^n \to \R^m$. Define
$\phi(x) := f(g(x))$. Then
$\nabla \phi(x) = \begin{bmatrix} \nabla g_1(x) & \ldots &
\nabla g_m(x) \end{bmatrix} \times \nabla f(z)|_{z=g(x)}$.

\textbf{Taylor series approximation}: given a function
$f(x): \R^n \to \R$ that is differentiable at
$x_0 \in \R^n$, it can be approximated by an affine
function in a neighborhood of $x_0$:
$f(x) = f(x_0) + \nabla f(x_0)^\top (x - x_0) + \epsilon(x)$,
where $\epsilon(x)$ goes to zero faster than first order,
i.e., $\lim_{x \to x_0} \frac{\epsilon(x)}{\|x-x_0\|} = 0$.
So, to the first order we have the approximation:
$f(x) \approx f(x_0) + \nabla f(x_0)^\top (x - x_0)$.

\subsection{Unconstrained Optimality Conditions}
Consider the optimization problem
$\min_{x \in \R^n} f(x)$, where $f$ is differentiable.

\textbf{First-order necessary condition}: If $x^*$ is a
local minimum, then $\nabla f(x^*) = 0$.

Suppose that $\nabla^2 f(x) \succeq 0$ for all
$x \in \R^n$, i.e., the problem is convex. Then:

All local minima are global minima.

$x^*$ is a global minimum (and a local minimum) if and only
if $\nabla f(x^*) = 0$.

\subsection{Slater's Condition}
Slater's condition is a widely used regularity condition.

Consider a convex problem $\min_x f_0(x)$ subject to
$f_i(x) = 0$ for $i = 1,\ldots,k$ and $h_j(x) \leq 0$ for
$j = 1,\ldots,m$. Denote the intersection of each $f_i$ and
each $h_j$'s domain as $\mathcal{D}$.

\textbf{Slater's condition} holds if there exists a point
$y \in \relint \mathcal{D}$ such that

$f_i(y) = 0$ for $i = 1,\ldots,k$.

$h_j(y) \leq 0$ for all affine $h_j$.

$h_j(y) < 0$ for all non-affine $f_j$.

$y$ is not unique in general.

When there are no constraints, Slater's condition holds by
convention.

When all constraints are affine, e.g., LP or QP, Slater's
condition is equivalent to feasibility. However,
\textbf{Slater's condition is stricter than feasibility in
general}.

\subsection{Constrained Optimality (KKT)}
Again, consider the optimization problem
$\min_x f_0(x)$ subject to $f_i(x) = 0$ for
$i = 1,\ldots,k$ and $h_j(x) \leq 0$ for $j = 1,\ldots,m$.

Denote the dual variables associated with the equality
constraints as $\mu_1,\ldots,\mu_k$. Similarly, denote the
dual variables associated with the inequality constraints as
$\lambda_1,\ldots,\lambda_m$. The Lagrangian of this problem
is then
$L(x,\lambda,\mu) := f(x) + \sum_{i=1}^k \mu_i f_i(x) +
\sum_{j=1}^m \lambda_j h_j(x)$.

\textbf{Karush--Kuhn--Tucker (KKT) conditions}: Consider
Lagrangian multipliers $\lambda_1^*, \ldots, \lambda_m^*$
and $\mu_1^*,\ldots,\mu_k^*$.

1. \textbf{Primal Feasibility}: $f_i(x^*) = 0$ for all
   $i = 1,\ldots,k$ and $h_j(x^*) \leq 0$ for all
   $j = 1,\ldots,m$;

2. \textbf{Dual Feasibility}: $\lambda_j^* \geq 0$ for all
   $j = 1,\ldots,m$;

3. \textbf{Lagrangian Stationarity}:
   $\nabla f(x^*) + \sum_{i=1}^k \mu_i^* \nabla f_i(x^*) +
   \sum_{j=1}^m \lambda_j^* \nabla h_j(x^*) = 0$;

4. \textbf{Complementary Slackness}:
   $\lambda_j^* \cdot h_j(x^*) = 0$ for all
   $j = 1,\ldots,m$.

For convex optimization problems that satisfy Slater's
condition, the KKT conditions are sufficient and necessary
can be used to find global optima.

\textbf{Example problem}: Consider a quadratic optimization
with equality constraints in the form of
$\min_{x \in \R^n} x^\top P_0 x + q_0^\top x + r_0$ subject
to $Ax = b$ where $P_0 \in \mathbb{S}^n_{++}$,
$A \in \R^{m \times n}$, $a_0 \in \R^n$, and $r_0 \in \R$.
Suppose that Slater's condition holds, i.e., $Ax = b$ admits
one or more solutions. Then, using the KKT condition, we can
show that the optimal primal-dual solution $(x^*, \mu^*)$
satisfies $\begin{bmatrix} A & 0_{m \times m} \\
2P_0 & A^\top \end{bmatrix} \begin{bmatrix} x^* \\
\mu^* \end{bmatrix} = \begin{bmatrix} b \\ -q_0
\end{bmatrix}$.
